{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL_ContrastiveLossPraktikum_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghXusr8ocUlr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchsummary import summary\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyperparameter batch size\n",
        "batch_size = 1024"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ElPDVS8rWNd"
      },
      "source": [
        "# Trying simple transformation to see if they have an impact\n",
        "#\n",
        "# -> Loss went down too quickly without learning additional features, therefore \n",
        "# not covered in our final experimental setup\n",
        "\n",
        "# Initialize transformations for DownloadDataset function\n",
        "transform_TensorOnly = T.Compose(\n",
        "    [T.ToTensor(),\n",
        "    T.Normalize((0.5), (0.5))\n",
        "])\n",
        "\n",
        "transform_simple = []\n",
        "transform_affine = []\n",
        "transform_FER = []\n",
        "transform_SimCLR = []\n",
        "\n",
        "def InitializeTransforms(sizevariable, colorjitter):\n",
        "    global transform_simple\n",
        "    global transform_affine\n",
        "    global transform_TensorOnly\n",
        "    global transform_FER\n",
        "    global transform_SimCLR\n",
        "    size = sizevariable\n",
        "    color_jitter = T.ColorJitter(0.8 * colorjitter, 0.8 * colorjitter, 0.8 * colorjitter, 0.2 * colorjitter)\n",
        "\n",
        "    transform_simple = T.Compose([\n",
        "        T.RandomHorizontalFlip(p=1),\n",
        "        T.RandomGrayscale(p=0.2),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "\n",
        "    transform_affine = T.Compose([\n",
        "        T.RandomHorizontalFlip(p=1),\n",
        "        T.RandomAffine((-30,+30)),\n",
        "        T.Resize(64),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "### Following three transformations are used in the report:\n",
        "\n",
        "    #Transformation for normalizing images only\n",
        "    transform_TensorOnly = T.Compose(\n",
        "        [T.ToTensor(),\n",
        "        T.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "\n",
        "    #Transformation from paper Guo, FER, 16\n",
        "    transform_FER = T.Compose([\n",
        "        T.Resize(sizevariable*2),\n",
        "        T.Pad(18),\n",
        "        T.RandomRotation((-15,+15)),\n",
        "        T.CenterCrop(sizevariable*2*1.125),\n",
        "        T.Resize(sizevariable),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "\n",
        "    #Transformation from paper Chen et al., SimCLR, 20\n",
        "    #size = sizevariable\n",
        "    #s=1\n",
        "    #color_jitter = T.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "    transform_SimCLR = T.Compose([\n",
        "        T.RandomResizedCrop(size=size),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomApply([color_jitter], p=0.8),\n",
        "        T.RandomGrayscale(p=0.2),\n",
        "        T.GaussianBlur(kernel_size=int(0.1 * 32)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5), (0.5))\n",
        "    ])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h56szH74BHPN"
      },
      "source": [
        "def DownloadDataset(dataset,transformation):\n",
        "    global sizevariable\n",
        "    global colorchannels\n",
        "    global classes\n",
        "    global train_dataset\n",
        "    global train_dataset_transformed\n",
        "    global test_dataset\n",
        "    if (dataset == \"MNIST\"):\n",
        "        classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "        train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_TensorOnly)\n",
        "        train_dataset_transformed = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transformation)\n",
        "        test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_TensorOnly)\n",
        "    elif (dataset == \"CIFAR10\"):\n",
        "        classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "        train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_TensorOnly)\n",
        "        train_dataset_transformed = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformation)\n",
        "        test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_TensorOnly)\n",
        "    else: \n",
        "        print(\"Download failed, please use MNIST or CIFAR10 as datasets.\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NZGlHtOErOp"
      },
      "source": [
        "Choose one of the following six download functions. These functions will overwrite each other if you run more than one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vI-bWsTDGkR"
      },
      "source": [
        "# Run for MNIST dataset with normalization only\n",
        "sizevariable = 28\n",
        "colorchannels = 1\n",
        "InitializeTransforms(sizevariable, 1)\n",
        "DownloadDataset(\"MNIST\", transform_TensorOnly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk8o73BbDpM5"
      },
      "source": [
        "# Run for MNIST dataset with FER image augmentation\n",
        "sizevariable = 28\n",
        "colorchannels = 1\n",
        "InitializeTransforms(sizevariable, 1)\n",
        "DownloadDataset(\"MNIST\", transform_FER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL8R7wJSDpHQ"
      },
      "source": [
        "# Run for MNIST dataset with SimCLR image augmentation\n",
        "sizevariable = 28\n",
        "colorchannels = 1\n",
        "InitializeTransforms(sizevariable, 1)\n",
        "DownloadDataset(\"MNIST\", transform_SimCLR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGTtpoZ4DpBc"
      },
      "source": [
        "# Run for CIFAR10 dataset with normalization only\n",
        "sizevariable = 32\n",
        "colorchannels = 3\n",
        "InitializeTransforms(sizevariable, 1)\n",
        "DownloadDataset(\"CIFAR10\", transform_TensorOnly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrh_CRGzDo7i"
      },
      "source": [
        "# Run for CIFAR10 dataset with FER image augmentation\n",
        "sizevariable = 32\n",
        "colorchannels = 3\n",
        "InitializeTransforms(sizevariable, 1)\n",
        "DownloadDataset(\"CIFAR10\", transform_FER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32LYQz8sDoxH"
      },
      "source": [
        "# Run for CIFAR10 dataset with SimCLR image augmentation\n",
        "sizevariable = 32\n",
        "colorchannels = 3\n",
        "InitializeTransforms(sizevariable, 1)\n",
        "DownloadDataset(\"CIFAR10\", transform_SimCLR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSah9Y9RlK9l"
      },
      "source": [
        "# Initiate empty lists for preparation\n",
        "for i in classes:\n",
        "    exec(f'train_dataset_{i}_prep = []')\n",
        "    exec(f'train_dataset_not_{i}_prep = []')\n",
        "    exec(f'train_dataset_{i}_transformed_prep = []')\n",
        "\n",
        "train_dataset_positive = []\n",
        "train_dataset_negative = []\n",
        "train_dataset_triplets = [] \n",
        "\n",
        "# Fill original and negative example lists per class\n",
        "for i in train_dataset:\n",
        "    train_dataset_classes = classes.copy()\n",
        "    for j in range(10):\n",
        "        if (i[1] == j):\n",
        "            exec(f'train_dataset_{train_dataset_classes[j]}_prep.append(i)')\n",
        "            del train_dataset_classes[j]\n",
        "            for k in range(9):\n",
        "                exec(f'train_dataset_not_{train_dataset_classes[k]}_prep.append(i)')\n",
        "\n",
        "# Fill transformed (positive example) lists per class\n",
        "for i in train_dataset_transformed:\n",
        "    for j in range(10):\n",
        "        if (i[1] == j):\n",
        "            exec(f'train_dataset_{classes[j]}_transformed_prep.append(i)') \n",
        "\n",
        "for i in classes:\n",
        "    # Shuffle the 'not' datasets; to have a mix of classes in the negative pairs / triplets\n",
        "    exec(f'random.shuffle(train_dataset_not_{i}_prep)')\n",
        "\n",
        "    # Create empty lists for each class (positive pairs, negative pairs and triplets)\n",
        "    exec(f'train_dataset_{i}_positive = []')\n",
        "    exec(f'train_dataset_{i}_negative = []')\n",
        "    exec(f'train_dataset_{i}_triplets = []')\n",
        "\n",
        "    # Index 1 for positive pairs and -1 for negative pairs, used in CosineEmbeddingLoss function\n",
        "    # Range 5000 to cut down the \"train_dataset_not_{i}_prep\" lists\n",
        "    for j in range(5000):\n",
        "        exec(f'train_dataset_{i}_positive.append((train_dataset_{i}_prep[{j}][0], train_dataset_{i}_transformed_prep[{j}][0], torch.tensor([1])))')\n",
        "        exec(f'train_dataset_{i}_negative.append((train_dataset_{i}_prep[{j}][0], train_dataset_not_{i}_prep[{j}][0], torch.tensor([-1])))')\n",
        "        exec(f'train_dataset_{i}_triplets.append((train_dataset_{i}_prep[{j}][0], train_dataset_{i}_transformed_prep[{j}][0], train_dataset_not_{i}_prep[{j}][0]))')\n",
        "\n",
        "    #Combine all lists \n",
        "    exec(f'train_dataset_positive.extend(train_dataset_{i}_positive)')\n",
        "    exec(f'train_dataset_negative.extend(train_dataset_{i}_negative)')\n",
        "    exec(f'train_dataset_triplets.extend(train_dataset_{i}_triplets)')\n",
        "\n",
        "# Positive pairs and negative pairs are combined as a last step\n",
        "train_dataset_pairwise = train_dataset_positive + train_dataset_negative"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V58E6i_a8FCG"
      },
      "source": [
        "# Initiate the data loaders\n",
        "train_loader_pairwise = torch.utils.data.DataLoader(train_dataset_pairwise, batch_size=batch_size, shuffle=True)\n",
        "train_loader_triplets = torch.utils.data.DataLoader(train_dataset_triplets, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# test_loader_total will be used later for evaluation\n",
        "test_loader_total = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2eIL1AeJ4es"
      },
      "source": [
        "You can use the following code to preview the first 16 images in one batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FsKytzZaVov"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# show examples of original images\n",
        "dataiter_og = iter(train_loader_triplets)\n",
        "images_og, transformed_og, negative_og = dataiter_og.next()\n",
        "imshow(torchvision.utils.make_grid(images_og[0:16]))\n",
        "imshow(torchvision.utils.make_grid(transformed_og[0:16]))\n",
        "imshow(torchvision.utils.make_grid(negative_og[0:16]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBMGxfUNVa8u"
      },
      "source": [
        "#FINAL for CIFAR10 and MNIST\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            nn.Conv2d(colorchannels, 32, 2, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 64, 2, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 2, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 2, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 8, 2, 1, 0, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.disc(input)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01u51uYaaaUT"
      },
      "source": [
        "# Set model based on dataset\n",
        "model_pairwise = CNN().to(device)\n",
        "model_triplets = CNN().to(device)\n",
        "\n",
        "# Define learning rates\n",
        "learning_rate_pairwise = 0.05\n",
        "learning_rate_triplets = 0.15\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion_pairwise = nn.CosineEmbeddingLoss()\n",
        "criterion_triplets = nn.TripletMarginLoss()\n",
        "optimizer_pairwise = torch.optim.SGD(model_pairwise.parameters(), lr=learning_rate_pairwise)\n",
        "optimizer_triplets = torch.optim.SGD(model_triplets.parameters(), lr=learning_rate_triplets)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FidJs5htKFdS"
      },
      "source": [
        "summary(model_triplets,(colorchannels,sizevariable,sizevariable))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pac4egUk9qm"
      },
      "source": [
        "# Helper functions to evaluate on the test\n",
        "#Please ignore details of code here; the individual steps reappear in the code later; and are documented there\n",
        "\n",
        "#Returns top-n-accuracy; adapted from: https://towardsdatascience.com/understanding-top-n-accuracy-metrics-8aa90170b35\n",
        "def top_n_accuracy(X,y,n,classifier):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "    clf = classifier\n",
        "    clf.fit(X_train,y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    probs = clf.predict_proba(X_test)\n",
        "    topn = np.argsort(probs, axis = 1)[:,-n:]\n",
        "    y_true = np.array(y_test)\n",
        "    return np.mean(np.array([1 if y_true[k] in topn[k] else 0 for k in range(len(topn))]))\n",
        "\n",
        "# Function to test the current network representation with test data by training a logistic regression\n",
        "# Prints accuracy and top-3-accuracy\n",
        "def evaluate(model):\n",
        "    targets = []\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in (test_loader_total):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels = labels.data.cpu().numpy()\n",
        "            for i in range(len(labels)):\n",
        "                label = labels[i]\n",
        "                targets.append(label)\n",
        "            outputs = model(images)\n",
        "            outputs = outputs.data.cpu().numpy()\n",
        "            for i in range(len(outputs)):\n",
        "                output = outputs[i]\n",
        "                features.append(output)\n",
        "    training_data, test_data, train_label, test_label = train_test_split(features, targets, train_size=0.8)\n",
        "    logreg = LogisticRegression(max_iter=1000)\n",
        "    logreg.fit(X=training_data, y=train_label)\n",
        "    pred_label=logreg.predict(test_data)\n",
        "    print('Accuracy Logistic Regression')\n",
        "    print(accuracy_score(test_label, pred_label))\n",
        "    top3accuracy = top_n_accuracy(features, targets, 3, LogisticRegression(max_iter=1000))\n",
        "    print('Top 3 accuracy')\n",
        "    print(top3accuracy)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUwGTBjta2Q1"
      },
      "source": [
        "# Number of epochs\n",
        "num_epochs_pairwise = 5\n",
        "num_epochs_triplets = 20\n",
        "\n",
        "# Steps only used for logging\n",
        "n_total_steps_pairwise = len(train_loader_pairwise)\n",
        "n_total_steps_triplets = len(train_loader_triplets)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoFGyFen2qwt"
      },
      "source": [
        "#Pairwise Training\n",
        "\n",
        "for epoch in range(num_epochs_pairwise):\n",
        "    if (epoch > 0):\n",
        "        evaluate(model_pairwise)\n",
        "    for i, (original, transformed, index) in enumerate(train_loader_pairwise):\n",
        "\n",
        "        original = original.to(device)\n",
        "        transformed = transformed.to(device)\n",
        "        index = index.flatten().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputO = model_pairwise(original)\n",
        "        outputT = model_pairwise(transformed)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion_pairwise(outputO, outputT, index)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer_pairwise.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_pairwise.step()\n",
        "\n",
        "        # Logging\n",
        "        if (i+1) % 30 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs_pairwise}], Step [{i+1}/{n_total_steps_pairwise}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nn8EaOma2EA"
      },
      "source": [
        "#Triplet Training\n",
        "\n",
        "for epoch in range(num_epochs_triplets):\n",
        "    if (epoch % 5 == 0) and (epoch > 0):\n",
        "        evaluate(model_triplets)\n",
        "    for i, (anchor, positive, negative) in enumerate(train_loader_triplets):\n",
        "\n",
        "        anchor = anchor.to(device)\n",
        "        positive = positive.to(device)\n",
        "        negative = negative.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputA = model_triplets(anchor)\n",
        "        outputP = model_triplets(positive)\n",
        "        outputN = model_triplets(negative)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion_triplets(outputA, outputP, outputN)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer_triplets.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_triplets.step()\n",
        "\n",
        "        # Logging\n",
        "        if (i+1) % 10 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs_triplets}], Step [{i+1}/{n_total_steps_triplets}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEdinvOqRJnm"
      },
      "source": [
        "#For evaluating the model, the model is applied to the test data\n",
        "\n",
        "def ModelTesting(model):\n",
        "    global targets\n",
        "    targets = [] # Targets will contain the class labels\n",
        "    global features\n",
        "    features = [] #Features will contain models output -> Contrastive Representation\n",
        "    with torch.no_grad(): \n",
        "        for images, labels in (test_loader_total):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels = labels.data.cpu().numpy()\n",
        "            for i in range(len(labels)): #Loop through one batch, append label to list of targets\n",
        "                label = labels[i]\n",
        "                targets.append(label)\n",
        "            outputs = model(images)\n",
        "            outputs = outputs.data.cpu().numpy() #Loop through one batch, append output to list of features\n",
        "            for i in range(len(outputs)):\n",
        "                output = outputs[i]\n",
        "                features.append(output)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HcpEeVxabfm"
      },
      "source": [
        "Choose one of the following validation options that you want to display via TSNE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnvxT6sOaRBn"
      },
      "source": [
        "# Run this for testing the pairwise-trained model\n",
        "ModelTesting(model_pairwise)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFbX552taQ0C"
      },
      "source": [
        "# Run this for testing the triplet-trained model\n",
        "ModelTesting(model_triplets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja2l0Dfyok_E"
      },
      "source": [
        "# Reduce number of features and targets to 1000 so that the visualization is not too cluttered\n",
        "features = np.array(features)\n",
        "features_cut = features[:1000,]\n",
        "targets = np.array(targets)\n",
        "targets_cut = targets[:1000,]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHlOkn8xQZuv"
      },
      "source": [
        "# After experimenting with different perplexities, we have decided to use 20 as 'main settings'\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=5000)\n",
        "tsne_results = tsne.fit_transform(features_cut) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5xt6XMhpinc"
      },
      "source": [
        "# Plot those points as a scatter plot and label them based on the labels (adapted from: https://towardsdatascience.com/visualizing-feature-vectors-embeddings-using-pca-and-t-sne-ef157cea3a42)\n",
        "cmap = cm.get_cmap('tab10')\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "num_categories = 10\n",
        "for lab in range(num_categories):\n",
        "    indices = targets_cut==lab\n",
        "    ax.scatter(tsne_results[indices,0],tsne_results[indices,1], c=np.array(cmap(lab)).reshape(1,4), label = classes[lab] ,alpha=0.5)\n",
        "ax.legend(fontsize='large', markerscale=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Dc2_AC3Nul"
      },
      "source": [
        "#We look at 6 different perplexity settings, to get a better idea of the representation.\n",
        "# -> Perplexity assumes the number of close neighbors each point has\n",
        "\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.subplots_adjust(top = 1.5)\n",
        "cmap = cm.get_cmap('tab10')\n",
        "for index, p in enumerate([3, 10, 15, 25, 35,50]):\n",
        "  tsne = TSNE(n_components = 2, perplexity = p, random_state=0, learning_rate=100, n_iter=5000)\n",
        "  tsne_results = tsne.fit_transform(features_cut)\n",
        "  num_categories = 10 \n",
        "  for lab in range(num_categories):\n",
        "    indices = targets_cut==lab\n",
        "    plt.subplot(2,3,index+1)\n",
        "    plt.scatter(tsne_results[indices,0],tsne_results[indices,1], c=np.array(cmap(lab)).reshape(1,4), label = lab ,alpha=0.5)\n",
        "    #plt.legend(fontsize='large', markerscale=2)\n",
        "    plt.title('Perplexity = '+ str(p)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNwGFvX20SxX"
      },
      "source": [
        "# t-SNE can be learned for three dimensions as well\n",
        "# The results can be plotted on a 3-dimensional scatterplot\n",
        "\n",
        "#Plot in 3D\n",
        "tsne = TSNE(3, verbose=1, perplexity=30, learning_rate=200) \n",
        "tsne_proj = tsne.fit_transform(features_cut) \n",
        "cmap = cm.get_cmap('tab10') \n",
        "fig = plt.figure(figsize=(8,8)) \n",
        "ax = fig.add_subplot(111, projection='3d') \n",
        "num_categories = 10 \n",
        "for lab in range(num_categories): \n",
        "  indices = targets_cut==lab \n",
        "  ax.scatter(tsne_proj[indices,0],tsne_proj[indices,1],tsne_proj[indices,2], c=np.array(cmap(lab)).reshape(1,4), label = classes[lab] ,alpha=0.5) \n",
        "ax.legend(fontsize='large', markerscale=2) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZcda8SqTVPc"
      },
      "source": [
        "#Attempt classification\n",
        "\n",
        "#Logistic Regression with scikit-learn\n",
        "\n",
        "#Features are the outputs of the trained model, applied to the test data (n = 10000)\n",
        "\n",
        "#We further split the test data into train (n = 2000) and test(n = 8000) (-> Train and test in regard to the classification)\n",
        "\n",
        "training_data, test_data, train_label, test_label = train_test_split(features, targets, train_size=0.8)\n",
        "#training_data, test_data, train_label, test_label = train_test_split(tsne_results, targets_test, train_size=0.8) #-> Train with T-SNE\n",
        "print(\"Training set size:\", len(training_data))\n",
        "print(\"Test set size:\", len(test_data))\n",
        "\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(X=training_data, y=train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfNuazTFPQvr"
      },
      "source": [
        "#Calculate the accuracy\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(X=training_data, y=train_label)\n",
        "\n",
        "pred_label=logreg.predict(test_data)\n",
        "  \n",
        "print('Accuracy Logistic Regression')\n",
        "print(accuracy_score(test_label, pred_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlpsomQPXsfi"
      },
      "source": [
        "#Confusion Matrix\n",
        "cm = confusion_matrix(test_label, pred_label)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3dMpInRZvJo"
      },
      "source": [
        "##Plot of confusion matrix (https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "np.set_printoptions(precision=3)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "# Plot non-normalized confusion matrix\n",
        "disp.plot(cmap = plt.cm.Blues, ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dFJkmJHJ6lc"
      },
      "source": [
        "#For getting a better idea of the prediction accuracy, we can look at the top-n-accuracy\n",
        "\n",
        "#Top N accuracy (https://towardsdatascience.com/understanding-top-n-accuracy-metrics-8aa90170b35)\n",
        "\n",
        "def top_n_accuracy(X,y,n,classifier):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  clf = classifier\n",
        "  clf.fit(X_train,y_train)\n",
        "  predictions = clf.predict(X_test)\n",
        "  probs = clf.predict_proba(X_test)\n",
        "  topn = np.argsort(probs, axis = 1)[:,-n:]\n",
        "  y_true = np.array(y_test)\n",
        "  return np.mean(np.array([1 if y_true[k] in topn[k] else 0 for k in range(len(topn))]))\n",
        "\n",
        "#Calculate top n accuracy -> Percentage of labels that lie in the top n predictions with highest probability\n",
        "n = 3\n",
        "acc = top_n_accuracy(features, targets, n, LogisticRegression(max_iter=1000))\n",
        "print(\"Top\", n, \"accuracy:\", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liXT_P34fPRY"
      },
      "source": [
        "#Do check, if the learning may have found nonlinear contrastive representations, we can also try knn, with varying amounts of labels\n",
        "\n",
        "#-> We have found, that using knn with n_neighbors between 20 and 100 usually slightly outperforms Logistic Regression, although not dramatically (1-3%) \n",
        "\n",
        "# Use this cell for your code\n",
        "knn = KNeighborsClassifier(n_neighbors=30)\n",
        "knn.fit(X=training_data, y=train_label)\n",
        "#Calculate accuracy\n",
        "pred_label=knn.predict(test_data)\n",
        "accuracy_score(test_label, pred_label)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}